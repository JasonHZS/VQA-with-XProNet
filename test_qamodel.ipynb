{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/vqa/VQA-with-XProNet/myvqa/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import evaluate\n",
    "import open_clip\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "from prototype import MultiThreadMemory\n",
    "from datasets import Dataset, load_metric\n",
    "from transformers import default_data_collator\n",
    "from open_clip import tokenizer as clip_tokenizer\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/v4.41.2/en/model_doc/bert#transformers.BertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "qa_bert = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384 # The maximum length of a feature (question and context)\n",
    "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2054, 2003, 2115, 2171, 1029, 102, 2026, 2171, 2003, 25353, 22144, 2378, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"What is your name?\", \"My name is Sylvain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA (GPU)\n"
     ]
    }
   ],
   "source": [
    "# Âà§Êñ≠Âπ∂ÈÄâÊã©ËÆæÂ§á\n",
    "def select_device():\n",
    "       if torch.cuda.is_available():\n",
    "              device = torch.device(\"cuda\")  # ‰ºòÂÖà‰ΩøÁî®CUDAÔºàNVIDIA GPUÔºâ\n",
    "              print(\"Using CUDA (GPU)\")\n",
    "       elif torch.backends.mps.is_available():\n",
    "              device = torch.device(\"mps\")  # Â¶ÇÊûúCUDA‰∏çÂèØÁî®‰ΩÜMPSÂèØÁî®Ôºå‰ΩøÁî®MPSÔºàApple SiliconÔºâ\n",
    "              print(\"Using MPS (Apple Silicon)\")\n",
    "       else:\n",
    "              device = torch.device(\"cpu\")  # Â¶ÇÊûúÈÉΩ‰∏çÂèØÁî®Ôºå‰ΩøÁî®CPU\n",
    "              print(\"Using CPU\")\n",
    "       return device\n",
    "\n",
    "device = select_device()\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ëá™ÂÆö‰πâÊï∞ÊçÆÈõÜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQADataset:\n",
    "    def __init__(self, key, questions, contexts, answers, img_names):\n",
    "        questions = questions\n",
    "        contexts = contexts\n",
    "        answers = answers\n",
    "        img_names = img_names\n",
    "        key = key\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'key': key[idx],\n",
    "            'question': questions[idx],\n",
    "            'context': contexts[idx],\n",
    "            'answers': answers[idx],\n",
    "            'image_name': img_names[idx]\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(questions)\n",
    " \n",
    " \n",
    "def load_datasets(data_dir, sub_folders, img_dir):\n",
    "    questions = []\n",
    "    contexts = []\n",
    "    answers = []\n",
    "    img_names = []\n",
    "    keys = []\n",
    "    for folder in sub_folders:\n",
    "        json_file_name = 'all_qs_dict_release_train_500.json' if 'train' in folder else 'all_qs_dict_release_test_500.json'\n",
    "        json_file_path = os.path.join(data_dir, folder, json_file_name)\n",
    "        with open(json_file_path) as f:\n",
    "            data = json.load(f)\n",
    "            for key in data.keys():\n",
    "                keys.append(key)\n",
    "                questions.append(data[key]['question'])\n",
    "                contexts.append(data[key]['fact_surface'].replace(\"[[\", \"\").replace(\"]]\", \"\"))\n",
    "                # Â§ÑÁêÜÁ≠îÊ°àÊ†ºÂºè\n",
    "                answer_text = data[key]['answer']\n",
    "                answer_start = contexts[-1].find(answer_text)  # ÈÄöËøáÊü•ÊâæÁ≠îÊ°àÂú®‰∏ä‰∏ãÊñá‰∏≠ÁöÑ‰ΩçÁΩÆ\n",
    "                answers.append({\n",
    "                    'answer_start': [answer_start if answer_start != -1 else 0],\n",
    "                    'text': [answer_text]\n",
    "                })\n",
    "                img_names.append(os.path.join(img_dir, data[key]['img_file']))\n",
    "    \n",
    "    # ÂàõÂª∫Hugging Face datasetsÂØπË±°\n",
    "    dataset = Dataset.from_dict({\n",
    "        'key': keys,\n",
    "        'question': questions,\n",
    "        'context': contexts,\n",
    "        'answers': answers,\n",
    "        'image_name': img_names\n",
    "    })\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': '270', 'question': 'Which object can be found in a jazz club', 'context': 'You are likely to find a trumpet in a jazz club', 'answers': {'answer_start': [25], 'text': ['trumpet']}, 'image_name': '/root/autodl-tmp/vqa/VQA-with-XProNet/data/KG_VQA/fvqa/exp_data/images/images/ILSVRC2012_test_00050748.JPEG'}\n",
      "ËÆ≠ÁªÉÈõÜÂ§ßÂ∞èÔºö 13662\n",
      "È™åËØÅÈõÜÂ§ßÂ∞èÔºö 13798\n"
     ]
    }
   ],
   "source": [
    "project_root = os.getcwd()\n",
    "train_data_dir = os.path.join(project_root, 'data/KG_VQA/fvqa/exp_data/train_seen_data')\n",
    "test_data_dir = os.path.join(project_root, 'data/KG_VQA/fvqa/exp_data/test_unseen_data')\n",
    "img_dir = os.path.join(project_root, \"data/KG_VQA/fvqa/exp_data/images/images\")\n",
    "sub_folders_train = ['train0', 'train1', 'train2', 'train3', 'train4']\n",
    "sub_folders_test = ['test0', 'test1', 'test2', 'test3', 'test4']\n",
    "\n",
    "train_dataset = load_datasets(train_data_dir, sub_folders_train, img_dir)\n",
    "# test_dataset_all = load_datasets(test_data_dir, sub_folders_test, img_dir)\n",
    "validation_dataset = load_datasets(test_data_dir, sub_folders_test, img_dir) \n",
    "# split_datasets = test_dataset_all.train_test_split(test_size=0.2)  \n",
    "# test_dataset = split_datasets['train']\n",
    "# validation_dataset = split_datasets['test']\n",
    "\n",
    "print(train_dataset[0])\n",
    "print('ËÆ≠ÁªÉÈõÜÂ§ßÂ∞èÔºö', len(train_dataset))\n",
    "# print('ÊµãËØïÈõÜÂ§ßÂ∞èÔºö', len(test_dataset))\n",
    "print('È™åËØÅÈõÜÂ§ßÂ∞èÔºö', len(validation_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Êï∞ÊçÆÂ§ÑÁêÜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "def prepare_train_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=256,\n",
    "       #  stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2029, 4874, 2064, 2022, 2179, 1999, 1037, 4166, 2252, 102, 2017, 2024, 3497, 2000, 2424, 1037, 9368, 1999, 1037, 4166, 2252, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2029, 4874, 1999, 2023, 3746, 2038, 1037, 5725, 102, 1037, 20497, 2038, 1037, 5725, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2054, 4874, 1999, 2023, 3746, 2003, 4141, 8828, 2005, 6265, 1029, 102, 1037, 11642, 2003, 1037, 7954, 4141, 8828, 2005, 6265, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2029, 4874, 1999, 2023, 3746, 2064, 3013, 2017, 102, 13227, 2064, 3013, 2017, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2029, 4874, 1999, 2023, 3746, 2064, 2022, 2109, 2005, 6276, 2477, 102, 1037, 5442, 2064, 2022, 2109, 2000, 3013, 2477, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'start_positions': [17, 11, 14, 10, 14], 'end_positions': [17, 11, 14, 10, 14]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = prepare_train_features(train_dataset[:5])\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13662/13662 [00:04<00:00, 3360.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(prepare_train_features, batched=True, remove_columns=train_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_test_dataset = test_dataset.map(prepare_train_features, batched=True, remove_columns=test_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/13798 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13798/13798 [00:04<00:00, 3209.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_val_dataset = validation_dataset.map(prepare_train_features, batched=True, remove_columns=validation_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "    num_rows: 13662\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA Train Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/vqa/VQA-with-XProNet/myvqa/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ËÆæÁΩÆËÆ≠ÁªÉÂèÇÊï∞\n",
    "model_name = \"qa-bert\"\n",
    "batch_size = 16\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-squad\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "data_collator = default_data_collator\n",
    "\n",
    "# ÂàùÂßãÂåñTrainer\n",
    "trainer = Trainer(\n",
    "    model=qa_bert,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    # eval_dataset ‰ªÖÁî®‰∫éËØÑ‰º∞Ê®°ÂûãÁöÑÊÄßËÉΩÔºå‰∏ç‰ºöÁî®‰∫éËÆ≠ÁªÉ„ÄÇÂÆÉÁöÑ‰∏ªË¶Å‰ΩúÁî®ÊòØÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÂÆöÊúüËØÑ‰º∞Ê®°ÂûãÔºå\n",
    "    # ‰ª•ÁõëÊéßÊ®°ÂûãÁöÑÊÄßËÉΩÂπ∂Èò≤Ê≠¢ËøáÊãüÂêà„ÄÇÂèØ‰ª•Âú®ËÆ≠ÁªÉÂêéÁªßÁª≠‰ΩøÁî® eval_dataset ËøõË°åËØÑ‰º∞ÂíåËÆ°ÁÆóÂêÑÁßçÊåáÊ†á„ÄÇ\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2562' max='2562' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2562/2562 29:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>0.076200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.041100</td>\n",
       "      <td>0.064588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>0.046178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2562, training_loss=0.0630003849828178, metrics={'train_runtime': 1793.4681, 'train_samples_per_second': 22.853, 'train_steps_per_second': 1.429, 'total_flos': 1.9031992389679104e+16, 'train_loss': 0.0630003849828178, 'epoch': 3.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ÂºÄÂßãËÆ≠ÁªÉ\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"test-squad-trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÈáçÊñ∞Âä†ËΩΩÊ®°Âûã\n",
    "trained_model = BertForQuestionAnswering.from_pretrained(\"test-squad-trained\")\n",
    "# ÈáçÊñ∞Âä†ËΩΩÂàÜËØçÂô®\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(\"test-squad-trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2425,  2033,  1996,  2171,  1997,  1996, 25381,  3491,  1999,\n",
      "          2023,  3746,  1029,   102, 22359,  7460,  2000,  1996,  4696,  1997,\n",
      "         25381,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "predict answer: lipstick\n",
      "true answer: {'answer_start': [0], 'text': ['lipstick']}\n"
     ]
    }
   ],
   "source": [
    "for key, question, context, answers in zip(validation_dataset['key'], validation_dataset['question'], validation_dataset['context'], validation_dataset['answers']):\n",
    "    # È¢ÑÊµãÁ≠îÊ°à\n",
    "    inputs = bert_tokenizer(question, context, return_tensors=\"pt\")\n",
    "    print(inputs)\n",
    "    with torch.no_grad():\n",
    "       outputs = trained_model(**inputs)\n",
    "       \n",
    "    answer_start_index = outputs.start_logits.argmax()\n",
    "    answer_end_index = outputs.end_logits.argmax()\n",
    "    \n",
    "    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "    predict_answer = tokenizer.decode(predict_answer_tokens, skip_special_tokens=True)\n",
    "    print(f\"predict answer: {predict_answer}\")\n",
    "    print(f\"true answer: {answers}\")\n",
    "    break \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'start_logits', 'end_logits'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in trainer.get_eval_dataloader():\n",
    "    break\n",
    "batch = {k: v.to(trainer.args.device) for k, v in batch.items()}\n",
    "with torch.no_grad():\n",
    "    output = trainer.model(**batch)\n",
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # We keep the example_id that gave us this feature and we will store the offset mappings.\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"key\"][sample_index])\n",
    "\n",
    "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "        # position is part of the context or not.\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13798/13798 [00:04<00:00, 2978.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "validation_features = validation_dataset.map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=validation_dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_predictions = trainer.predict(validation_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_qa_predictions(examples, features, raw_predictions, squad_v2=True, n_best_size = 20, max_answer_length = 30):\n",
    "    \"\"\"\n",
    "    ËØ•ÂáΩÊï∞ÂèØ‰ª•Áî±tokenizer.decode ÂÆåÊàêËæìÂá∫ÊúÄÁªàÁöÑÁ≠îÊ°àÊñáÊú¨„ÄÇ\n",
    "    \"\"\"\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    # Build a map example to its corresponding features.\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"key\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # The dictionaries we have to fill.\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    # Logging.\n",
    "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
    "\n",
    "    # Let's loop over all the examples!\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # Those are the indices of the features associated to the current example.\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_score = None # Only used if squad_v2 is True.\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example[\"context\"]\n",
    "        # Looping through all the features associated to the current example.\n",
    "        for feature_index in feature_indices:\n",
    "            # We grab the predictions of the model for this feature.\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
    "            # context.\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # Update minimum null prediction.\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
    "                    # to part of the input_ids that are not in the context.\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
    "            # failure.\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n",
    "        if not squad_v2:\n",
    "            predictions[example[\"key\"]] = best_answer[\"text\"]\n",
    "        else:\n",
    "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
    "            predictions[example[\"key\"]] = answer\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 13798 example predictions split into 13798 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13798/13798 [00:28<00:00, 477.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# squad_v2=TrueÂÖÅËÆ∏Ê®°ÂûãÂú®Êó†Ê≥ï‰ªécontext‰∏≠ÊâæÂà∞answerÊó∂,È¢ÑÊµã\"‰∏çÂèØËÉΩÂõûÁ≠î\",‰ªéËÄåÂ§ÑÁêÜËøôÁßçÊÉÖÂÜµ„ÄÇ\n",
    "squad_v2 = False\n",
    "final_predictions = postprocess_qa_predictions(validation_dataset, validation_features, raw_predictions.predictions, squad_v2=squad_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('271', 'lipstick'),\n",
       "             ('3519', 'bus'),\n",
       "             ('3518', 'pizza'),\n",
       "             ('3513', 'lizard'),\n",
       "             ('3517', 'pizza'),\n",
       "             ('3516', '*Something you find on a pizza is cheese'),\n",
       "             ('3515', 'dog'),\n",
       "             ('3514', 'temporary residence'),\n",
       "             ('98', 'luggage'),\n",
       "             ('91', 'glove'),\n",
       "             ('93', 'pineapple'),\n",
       "             ('92', 'pineapple'),\n",
       "             ('1176', 'carrot'),\n",
       "             ('877', 'bus'),\n",
       "             ('3432', 'refrigerator'),\n",
       "             ('623', 'laptop'),\n",
       "             ('875', 'motorcycle'),\n",
       "             ('622', 'An ipod is for listening'),\n",
       "             ('872', 'a bicycle'),\n",
       "             ('3435', 'Dogs'),\n",
       "             ('627', 'bus'),\n",
       "             ('4593', 'Baseball'),\n",
       "             ('624', 'bus'),\n",
       "             ('4591', 'baseball'),\n",
       "             ('2745', 'cat'),\n",
       "             ('2747', 'cat'),\n",
       "             ('2749', 'cat'),\n",
       "             ('4845', 'goldfish'),\n",
       "             ('4735', 'pizza'),\n",
       "             ('2039', 'banana'),\n",
       "             ('4738', 'pizza'),\n",
       "             ('391', 'wild'),\n",
       "             ('396', 'bench'),\n",
       "             ('394', 'chocolate is related to cake'),\n",
       "             ('399', 'dogs'),\n",
       "             ('398', 'dog'),\n",
       "             ('2309', 'snake'),\n",
       "             ('4729', 'horse'),\n",
       "             ('2301', 'car'),\n",
       "             ('2300', 'car'),\n",
       "             ('2302', 'cars'),\n",
       "             ('2305', 'microwave'),\n",
       "             ('4723', 'lemon'),\n",
       "             ('5847', 'corkscrew'),\n",
       "             ('5846', 'baseball'),\n",
       "             ('5845', 'baseball'),\n",
       "             ('5844', 'baseball glove'),\n",
       "             ('5840', 'baseball'),\n",
       "             ('5849', 'cheese'),\n",
       "             ('5848', 'corkscrew'),\n",
       "             ('3748', 'Bananas'),\n",
       "             ('3749', 'banana'),\n",
       "             ('3746', 'tie'),\n",
       "             ('3742', 'basketball'),\n",
       "             ('3743', 'pillows'),\n",
       "             ('3740', 'handbag'),\n",
       "             ('3741', 'handbag'),\n",
       "             ('4627', 'position'),\n",
       "             ('178', 'stripes'),\n",
       "             ('177', 'an umbrella'),\n",
       "             ('174', 'weddings'),\n",
       "             ('172', 'clock'),\n",
       "             ('1985', 'glass'),\n",
       "             ('1769', 'wall'),\n",
       "             ('1765', 'lipstick'),\n",
       "             ('1547', 'horse'),\n",
       "             ('696', 'preventing from getting wet'),\n",
       "             ('697', 'microwave'),\n",
       "             ('694', 'beer'),\n",
       "             ('1541', 'dog'),\n",
       "             ('698', 'refrigerator'),\n",
       "             ('699', 'motorcycle'),\n",
       "             ('1548', 'bench'),\n",
       "             ('1549', 'countryside is horses'),\n",
       "             ('5618', 'tourists'),\n",
       "             ('5612', 'plates'),\n",
       "             ('5613', 'Cakes'),\n",
       "             ('5610', 'elephant'),\n",
       "             ('5611', 'dogs'),\n",
       "             ('5617', 'elephants'),\n",
       "             ('5615', 'baseball glove'),\n",
       "             ('4668', 'ray'),\n",
       "             ('258', 'carrot'),\n",
       "             ('259', 'carrot'),\n",
       "             ('252', 'cheese'),\n",
       "             ('256', 'book'),\n",
       "             ('257', 'book'),\n",
       "             ('3680', 'elephant'),\n",
       "             ('2513', 'glass'),\n",
       "             ('3531', 'lemon'),\n",
       "             ('3533', 'lemon'),\n",
       "             ('3532', 'Lemon'),\n",
       "             ('3535', 'motorcycle'),\n",
       "             ('3537', 'clouds'),\n",
       "             ('3536', 'motorcycle'),\n",
       "             ('5056', 'piano'),\n",
       "             ('5051', 'remote'),\n",
       "             ('1158', 'snake'),\n",
       "             ('1157', 'snake'),\n",
       "             ('1156', 'snake'),\n",
       "             ('1151', 'computer'),\n",
       "             ('1152', 'dogs'),\n",
       "             ('4298', 'tick'),\n",
       "             ('4294', 'sand'),\n",
       "             ('4291', 'sand'),\n",
       "             ('4290', 'snake'),\n",
       "             ('4293', 'sand'),\n",
       "             ('4292', 'sand'),\n",
       "             ('4749', 'refrigerator'),\n",
       "             ('1324', 'motorcycle'),\n",
       "             ('4743', 'laptop'),\n",
       "             ('4742', 'glass'),\n",
       "             ('4746', 'bottle'),\n",
       "             ('4745', 'laptop'),\n",
       "             ('4744', 'laptop'),\n",
       "             ('5867', 'Dogs'),\n",
       "             ('5866', 'hot dogs'),\n",
       "             ('5861', 'sunglasses'),\n",
       "             ('5860', 'elephant'),\n",
       "             ('2183', 'dog'),\n",
       "             ('2180', 'bagels'),\n",
       "             ('2187', 'motorcycle'),\n",
       "             ('2186', 'motorcycle'),\n",
       "             ('2185', 'Dogs'),\n",
       "             ('4606', 'dog'),\n",
       "             ('4607', 'dogs'),\n",
       "             ('2189', 'motorcycle'),\n",
       "             ('2188', 'motorcycle'),\n",
       "             ('150', 'dragonflies'),\n",
       "             ('157', 'jellyfish'),\n",
       "             ('159', 'kitchen'),\n",
       "             ('158', 'jellyfish'),\n",
       "             ('2726', 'a runway is'),\n",
       "             ('3089', 'train'),\n",
       "             ('3088', 'trains'),\n",
       "             ('810', 'throw'),\n",
       "             ('811', 'Cars'),\n",
       "             ('812', 'glove'),\n",
       "             ('813', 'jazz blues'),\n",
       "             ('814', 'cat'),\n",
       "             ('1700', 'boat'),\n",
       "             ('1701', 'boats'),\n",
       "             ('5264', 'orange'),\n",
       "             ('1704', 'basketball'),\n",
       "             ('1705', 'basketball'),\n",
       "             ('1707', 'basketball'),\n",
       "             ('5268', 'motorcycle'),\n",
       "             ('4574', 'computer'),\n",
       "             ('4575', 'computer'),\n",
       "             ('3959', 'bus'),\n",
       "             ('3029', 'train'),\n",
       "             ('3028', 'train'),\n",
       "             ('3027', 'train'),\n",
       "             ('3026', 'trains'),\n",
       "             ('3024', 'pizza'),\n",
       "             ('3023', 'goldfish'),\n",
       "             ('3022', 'Fire hydrants'),\n",
       "             ('3021', 'fire hydrant'),\n",
       "             ('4198', 'bird'),\n",
       "             ('4199', 'bird'),\n",
       "             ('4196', 'bird'),\n",
       "             ('4197', 'bird'),\n",
       "             ('4194', 'bird'),\n",
       "             ('4195', 'bird'),\n",
       "             ('4192', 'bird'),\n",
       "             ('4193', 'bird'),\n",
       "             ('4190', 'bird'),\n",
       "             ('4191', 'bird'),\n",
       "             ('5671', 'pillows'),\n",
       "             ('5673', 'harp'),\n",
       "             ('5677', 'refrigerator'),\n",
       "             ('5678', 'monitor'),\n",
       "             ('5679', 'monitor'),\n",
       "             ('238', 'plunger'),\n",
       "             ('239', 'toilet'),\n",
       "             ('234', 'orange'),\n",
       "             ('235', 'airport'),\n",
       "             ('230', 'orange'),\n",
       "             ('231', 'orange'),\n",
       "             ('232', 'orange'),\n",
       "             ('233', 'banana'),\n",
       "             ('2532', 'Horses'),\n",
       "             ('2531', 'horse'),\n",
       "             ('2534', 'skateboard'),\n",
       "             ('5070', 'Dragonfly'),\n",
       "             ('939', 'donut'),\n",
       "             ('933', 'bus'),\n",
       "             ('931', 'boat'),\n",
       "             ('937', 'snow'),\n",
       "             ('5384', 'bottle'),\n",
       "             ('5386', 'bottle'),\n",
       "             ('5387', 'computer'),\n",
       "             ('5388', 'computer'),\n",
       "             ('5389', 'laptop'),\n",
       "             ('795', 'Pineapple'),\n",
       "             ('794', 'pineapple'),\n",
       "             ('793', 'pineapple'),\n",
       "             ('792', 'pineapple'),\n",
       "             ('791', 'Pineapple'),\n",
       "             ('790', 'pineapple'),\n",
       "             ('2145', 'plates'),\n",
       "             ('798', 'a cello is like a violin'),\n",
       "             ('2142', 'hot dog'),\n",
       "             ('2141', 'monitor'),\n",
       "             ('1133', 'desk'),\n",
       "             ('1132', 'monitor'),\n",
       "             ('1131', 'Baseball'),\n",
       "             ('1137', 'dining table'),\n",
       "             ('2419', 'Horse'),\n",
       "             ('4975', 'luggage'),\n",
       "             ('2887', 'travel'),\n",
       "             ('2', 'a dog is for'),\n",
       "             ('1921', 'books'),\n",
       "             ('1922', 'Books'),\n",
       "             ('1923', 'computer'),\n",
       "             ('1924', 'computer'),\n",
       "             ('1925', 'desk'),\n",
       "             ('1926', 'keyboard'),\n",
       "             ('4761', 'horses'),\n",
       "             ('4760', 'keyboard'),\n",
       "             ('4763', 'motorcycle are more common than car'),\n",
       "             ('4762', 'skateboard'),\n",
       "             ('4765', 'pineapple'),\n",
       "             ('4767', 'hotdog'),\n",
       "             ('4766', 'luggage'),\n",
       "             ('4769', 'luggage'),\n",
       "             ('4768', 'train'),\n",
       "             ('5809', 'keyboard'),\n",
       "             ('5808', 'desk'),\n",
       "             ('5803', 'banana'),\n",
       "             ('1572', 'cat'),\n",
       "             ('1571', 'Cats'),\n",
       "             ('1570', 'cars'),\n",
       "             ('1577', 'harp'),\n",
       "             ('1575', 'pen'),\n",
       "             ('4888', 'banana'),\n",
       "             ('4880', 'Benches'),\n",
       "             ('4881', 'Benches'),\n",
       "             ('4882', 'Benches'),\n",
       "             ('2703', 'motorcycle'),\n",
       "             ('4887', 'banana'),\n",
       "             ('475', 'Broccoli'),\n",
       "             ('3474', 'cat'),\n",
       "             ('3475', 'Cats'),\n",
       "             ('3472', 'lipstick'),\n",
       "             ('838', 'piano'),\n",
       "             ('5249', 'cat'),\n",
       "             ('5248', 'cats'),\n",
       "             ('1728', 'laptop'),\n",
       "             ('1729', 'motorcycle'),\n",
       "             ('1726', 'Bows'),\n",
       "             ('1724', 'cow'),\n",
       "             ('1725', 'hair spray'),\n",
       "             ('1720', 'Zebra'),\n",
       "             ('1721', 'Baseball'),\n",
       "             ('3005', 'cat'),\n",
       "             ('3006', 'plates'),\n",
       "             ('3001', 'pen'),\n",
       "             ('3003', 'cats'),\n",
       "             ('3315', 'Violins'),\n",
       "             ('1580', 'microwave'),\n",
       "             ('1584', 'luggage'),\n",
       "             ('1585', 'backpack'),\n",
       "             ('5481', 'snake'),\n",
       "             ('5480', 'elephants'),\n",
       "             ('5658', 'Horses'),\n",
       "             ('5659', 'horse'),\n",
       "             ('5656', 'cars'),\n",
       "             ('5654', 'bus'),\n",
       "             ('5655', 'bus'),\n",
       "             ('3917', 'butterfly'),\n",
       "             ('3916', 'butterfly'),\n",
       "             ('3915', 'butterfly'),\n",
       "             ('3914', 'butterfly'),\n",
       "             ('3913', 'elephant'),\n",
       "             ('3912', 'elephant'),\n",
       "             ('3911', 'elephant'),\n",
       "             ('3910', 'elephant'),\n",
       "             ('3918', 'butterfly'),\n",
       "             ('3287', 'Cars'),\n",
       "             ('216', 'laptop'),\n",
       "             ('214', 'pizza'),\n",
       "             ('212', 'balance beam'),\n",
       "             ('210', 'bus'),\n",
       "             ('219', 'microwave'),\n",
       "             ('3649', 'Strawberries'),\n",
       "             ('3647', 'piano'),\n",
       "             ('3646', 'banana'),\n",
       "             ('3644', 'Cakes'),\n",
       "             ('2550', 'Dogs'),\n",
       "             ('2551', 'skateboard'),\n",
       "             ('2552', 'skateboard'),\n",
       "             ('2553', 'skateboarding'),\n",
       "             ('2554', 'zebra'),\n",
       "             ('2555', 'zebra'),\n",
       "             ('2556', 'Zebra'),\n",
       "             ('2557', 'cello'),\n",
       "             ('2558', 'cello'),\n",
       "             ('2559', 'cello'),\n",
       "             ('5013', 'ant'),\n",
       "             ('5014', 'car'),\n",
       "             ('5017',\n",
       "              '*Something you find on the surface of the earth is sand'),\n",
       "             ('5018', 'sand'),\n",
       "             ('5019', 'banana'),\n",
       "             ('4419', 'orange'),\n",
       "             ('4418', 'Lemon'),\n",
       "             ('4417', 'horses'),\n",
       "             ('4416', 'horses'),\n",
       "             ('4413', 'zoo'),\n",
       "             ('4410', 'pizza'),\n",
       "             ('918', 'camel'),\n",
       "             ('914', 'Broccoli'),\n",
       "             ('910', 'trombone'),\n",
       "             ('912', 'Cats'),\n",
       "             ('630', 'squirrels'),\n",
       "             ('631', 'squirrels'),\n",
       "             ('1210', 'dining table'),\n",
       "             ('633', 'squirrel'),\n",
       "             ('635', 'watch'),\n",
       "             ('1117', 'computer'),\n",
       "             ('1116', 'cake'),\n",
       "             ('1119', 'computer'),\n",
       "             ('1118', 'computer'),\n",
       "             ('2009', 'snowmobile'),\n",
       "             ('4853', 'broccoli'),\n",
       "             ('4725', 'elephant'),\n",
       "             ('4583', 'motorcycle'),\n",
       "             ('4724', 'lemon'),\n",
       "             ('4582', 'pavement'),\n",
       "             ('4721', 'clock'),\n",
       "             ('2307', 'microwave'),\n",
       "             ('5821', 'bus'),\n",
       "             ('5820', 'plane'),\n",
       "             ('5823', 'boat'),\n",
       "             ('5822', 'boat'),\n",
       "             ('194', 'cheese'),\n",
       "             ('191', 'unicycle'),\n",
       "             ('193', 'scissors'),\n",
       "             ('2769', 'Zebra'),\n",
       "             ('2768', 'Zebra'),\n",
       "             ('2763', 'dog'),\n",
       "             ('2767', 'zebra'),\n",
       "             ('2766', 'sand'),\n",
       "             ('2765', 'dog poop'),\n",
       "             ('2764', 'dog'),\n",
       "             ('3451', 'skateboarding'),\n",
       "             ('3457', 'driving'),\n",
       "             ('859', 'vase'),\n",
       "             ('5228', 'cattle'),\n",
       "             ('5223', 'position'),\n",
       "             ('5222', 'position'),\n",
       "             ('5227', 'cow'),\n",
       "             ('5226', 'cow'),\n",
       "             ('4588', 'violin'),\n",
       "             ('3067', 'dog'),\n",
       "             ('3066', 'dog'),\n",
       "             ('3065', 'dog'),\n",
       "             ('3064', 'backpack'),\n",
       "             ('3069', 'dog'),\n",
       "             ('3068', 'dog'),\n",
       "             ('1304', 'elephant'),\n",
       "             ('1305', 'light'),\n",
       "             ('1306', 'orange'),\n",
       "             ('1307', 'bell pepper'),\n",
       "             ('1301', 'sunglasses'),\n",
       "             ('1303', 'horse'),\n",
       "             ('1308', 'Bell pepper'),\n",
       "             ('499', 'refrigerator'),\n",
       "             ('494', 'harmonica'),\n",
       "             ('495', 'harmonica'),\n",
       "             ('490', 'You can use a frisbee to play with your dog'),\n",
       "             ('492', 'monitor'),\n",
       "             ('493', 'Broccoli'),\n",
       "             ('3939', 'cat'),\n",
       "             ('3934', 'iPod'),\n",
       "             ('3937', 'computer'),\n",
       "             ('3936', 'computer'),\n",
       "             ('3931', 'dog'),\n",
       "             ('3933', 'laptop'),\n",
       "             ('3265', 'Airports have runways'),\n",
       "             ('4044', 'violin'),\n",
       "             ('4045', 'violins'),\n",
       "             ('4046', 'Violins'),\n",
       "             ('4047', 'violin'),\n",
       "             ('4041', 'vehicles'),\n",
       "             ('4042', 'motorcycle'),\n",
       "             ('4043', 'violin'),\n",
       "             ('3665', 'pizza'),\n",
       "             ('3666', 'pizza'),\n",
       "             ('3663', 'Cats'),\n",
       "             ('3669', 'motorcycle'),\n",
       "             ('2578', 'strawberry'),\n",
       "             ('2579', 'strawberry'),\n",
       "             ('2576', 'strawberry'),\n",
       "             ('2577', 'strawberry'),\n",
       "             ('2574', 'strawberry'),\n",
       "             ('2575', 'strawberry'),\n",
       "             ('2572', 'strawberry'),\n",
       "             ('2570', 'strawberry'),\n",
       "             ('2571', 'strawberry'),\n",
       "             ('3593', 'truck'),\n",
       "             ('3592', 'truck'),\n",
       "             ('3591', 'Trucks'),\n",
       "             ('3597', 'cow'),\n",
       "             ('3596', 'cow'),\n",
       "             ('3595', 'cow'),\n",
       "             ('3594', 'truck'),\n",
       "             ('5037', 'truck'),\n",
       "             ('5034', 'temporary residence'),\n",
       "             ('4434', 'Hot dogs'),\n",
       "             ('4437', 'bottle'),\n",
       "             ('4436', 'hot dog'),\n",
       "             ('4431', 'train are cheaper than flight'),\n",
       "             ('4439', 'computer'),\n",
       "             ('975', 'cut'),\n",
       "             ('972', 'You are likely to find a bowl in a kitchen'),\n",
       "             ('971', 'computer'),\n",
       "             ('970', 'rubber eraser'),\n",
       "             ('3386', 'laptop'),\n",
       "             ('3387', 'cat'),\n",
       "             ('3385', 'pizza'),\n",
       "             ('1681', 'buildings'),\n",
       "             ('1683', 'buildings'),\n",
       "             ('1682', 'Motorcycles'),\n",
       "             ('1686', 'snowmobile'),\n",
       "             ('1469', 'desk'),\n",
       "             ('1468', 'a Monitor'),\n",
       "             ('1464', 'remote'),\n",
       "             ('1467', 'monitor'),\n",
       "             ('1466', 'Computers'),\n",
       "             ('4239', 'donut'),\n",
       "             ('4238', 'doughnut'),\n",
       "             ('4237', 'donut'),\n",
       "             ('4236', 'donut'),\n",
       "             ('4235', 'buildings'),\n",
       "             ('4234', 'cow'),\n",
       "             ('4233', 'horse'),\n",
       "             ('4232', 'horses'),\n",
       "             ('4230', 'boat'),\n",
       "             ('5590', 'monitor'),\n",
       "             ('5593', 'weddings'),\n",
       "             ('5595', 'Weddings'),\n",
       "             ('5594', 'weddings are happy for the'),\n",
       "             ('5597', 'keyboard'),\n",
       "             ('5596', 'weddings'),\n",
       "             ('5598', 'keyboard'),\n",
       "             ('311', 'hats'),\n",
       "             ('310', 'an office can be used for working'),\n",
       "             ('317', 'bookshelf'),\n",
       "             ('316', 'bookshelf contains books'),\n",
       "             ('2299', 'car'),\n",
       "             ('2293', 'sunglasses'),\n",
       "             ('2292', 'sunglasses'),\n",
       "             ('2291', 'sunglasses'),\n",
       "             ('2290', 'sunglasses'),\n",
       "             ('2297', 'vehicles'),\n",
       "             ('5190', 'skateboard'),\n",
       "             ('5578', 'motorcycle'),\n",
       "             ('4672', 'bedroom'),\n",
       "             ('3439', 'The kingdom'),\n",
       "             ('3431', 'refrigerator'),\n",
       "             ('873', 'unicycle'),\n",
       "             ('3436', 'dogs'),\n",
       "             ('3437', 'dog'),\n",
       "             ('2741', 'elephant'),\n",
       "             ('2743', 'hot dog'),\n",
       "             ('2742', 'Hot dogs'),\n",
       "             ('4594', 'baseball'),\n",
       "             ('4844', 'goldfish'),\n",
       "             ('2748', 'cat'),\n",
       "             ('4846', 'goldfish'),\n",
       "             ('4840', 'piano'),\n",
       "             ('4841', 'piano'),\n",
       "             ('4842', 'goldfish'),\n",
       "             ('5201', 'cake'),\n",
       "             ('5200', 'cake'),\n",
       "             ('5203', 'punnet is related'),\n",
       "             ('5202', 'cake'),\n",
       "             ('5204', 'strawberries'),\n",
       "             ('3041', 'motorcycle'),\n",
       "             ('3040', 'motorcycle'),\n",
       "             ('3043', 'motorcycle'),\n",
       "             ('3044', 'bus'),\n",
       "             ('3047', 'restaurant are a bit more expensive than cafe'),\n",
       "             ('1094', 'iPod'),\n",
       "             ('1096', 'iPod'),\n",
       "             ('1097', 'bus'),\n",
       "             ('1090', 'roads'),\n",
       "             ('1092', 'dog'),\n",
       "             ('1098', 'bus'),\n",
       "             ('1099', 'elephant'),\n",
       "             ('1322', 'motorcycle'),\n",
       "             ('1323', 'car'),\n",
       "             ('1320', 'motorcycle'),\n",
       "             ('1321', 'motorcycle'),\n",
       "             ('1326', 'road'),\n",
       "             ('1914', 'book'),\n",
       "             ('1325', 'motorcycle'),\n",
       "             ('1328', 'elephant'),\n",
       "             ('1917', 'Glasses'),\n",
       "             ('3958', 'bus'),\n",
       "             ('3952', 'living room can be warmer than bedroom'),\n",
       "             ('3951', 'zebra'),\n",
       "             ('3957', 'bus'),\n",
       "             ('3956', 'Buses'),\n",
       "             ('3955', 'bus'),\n",
       "             ('1546', 'horses'),\n",
       "             ('4067', 'car'),\n",
       "             ('4068', 'skateboard'),\n",
       "             ('4069', 'skateboard'),\n",
       "             ('5787', 'bench'),\n",
       "             ('5780', 'Lizards'),\n",
       "             ('5781', 'Lizards'),\n",
       "             ('5782', 'lizard'),\n",
       "             ('3603', 'train'),\n",
       "             ('3602', 'pizza'),\n",
       "             ('3601', 'pizza'),\n",
       "             ('3600', 'pizza'),\n",
       "             ('3607', 'horse'),\n",
       "             ('3609', 'iPod'),\n",
       "             ('3608', 'remote'),\n",
       "             ('4452', 'boy'),\n",
       "             ('4450', 'skateboard'),\n",
       "             ('4456', 'laptop'),\n",
       "             ('4455', 'toilet'),\n",
       "             ('4454', 'toilet paper'),\n",
       "             ('4459', 'car'),\n",
       "             ('4458', 'sand'),\n",
       "             ('959', 'watching'),\n",
       "             ('958', 'cell phone'),\n",
       "             ('955', 'keyboard'),\n",
       "             ('1544', 'bus'),\n",
       "             ('717', 'laptop'),\n",
       "             ('2178', 'Bagels'),\n",
       "             ('4964', 'computer'),\n",
       "             ('1669', 'dining table'),\n",
       "             ('1668', 'banana'),\n",
       "             ('1667', 'banana'),\n",
       "             ('1666', 'banana'),\n",
       "             ('1665', 'orange'),\n",
       "             ('1664', 'bell pepper'),\n",
       "             ('1662', 'cow'),\n",
       "             ('1661', 'cow'),\n",
       "             ('1660', 'cow'),\n",
       "             ('590', 'clock'),\n",
       "             ('592', 'camel'),\n",
       "             ('598', 'trains'),\n",
       "             ('2828', 'sunglasses'),\n",
       "             ('2829', 'a patio is used'),\n",
       "             ('2821', 'cell phone'),\n",
       "             ('5414', 'snowmobile'),\n",
       "             ('5417', 'elephant'),\n",
       "             ('3383', 'salad'),\n",
       "             ('3381', 'fesh vegtables'),\n",
       "             ('5418', 'elephant'),\n",
       "             ('3388', 'monitor'),\n",
       "             ('3389', 'Cats'),\n",
       "             ('1403', 'toilet'),\n",
       "             ('1402', 'a bathroom is for'),\n",
       "             ('1400', 'jellyfish'),\n",
       "             ('1407', 'monitor'),\n",
       "             ('1405', 'zebra'),\n",
       "             ('1404', 'a bathroom is for'),\n",
       "             ('4215', 'cow'),\n",
       "             ('4214', 'cow'),\n",
       "             ('4216', 'cattle'),\n",
       "             ('693', 'zebra'),\n",
       "             ('4219', 'mountainous area'),\n",
       "             ('1545', 'horses'),\n",
       "             ('1542', 'dog'),\n",
       "             ('1543', 'Dogs'),\n",
       "             ('1540', 'large container'),\n",
       "             ('3260', 'cow'),\n",
       "             ('5577', 'motorcycle'),\n",
       "             ('5576', 'elephants'),\n",
       "             ('5575', 'elephants'),\n",
       "             ('5572', 'baseball glove'),\n",
       "             ('3262', 'keyboard'),\n",
       "             ('5799', 'Horses'),\n",
       "             ('1937', 'sand'),\n",
       "             ('3267', 'dogs'),\n",
       "             ('335', 'oranges are sweet'),\n",
       "             ('337', 'toilet'),\n",
       "             ('3266', 'Dogs'),\n",
       "             ('3269', 'harp'),\n",
       "             ('5127', 'bus'),\n",
       "             ('5002', 'cat'),\n",
       "             ('3195', 'luggage'),\n",
       "             ('1497', 'Bananas'),\n",
       "             ('3416', 'monitor'),\n",
       "             ('3417', 'desk'),\n",
       "             ('3414', 'computer'),\n",
       "             ('3415', 'keyboard'),\n",
       "             ('3412', 'harmonica'),\n",
       "             ('3413', 'monitor'),\n",
       "             ('3410', 'harmonica'),\n",
       "             ('3411', 'harmonica'),\n",
       "             ('3419', 'desk'),\n",
       "             ('4868', 'elephant'),\n",
       "             ('4869', 'kitten'),\n",
       "             ('2019', 'horse'),\n",
       "             ('2018', 'strawberry'),\n",
       "             ('4862', 'laptop'),\n",
       "             ('2014', 'elephants'),\n",
       "             ('2016', 'elephants'),\n",
       "             ('2010', 'wall'),\n",
       "             ('4864', 'train'),\n",
       "             ('4890', 'pretzel'),\n",
       "             ('458', 'spoon'),\n",
       "             ('1349', 'laptop'),\n",
       "             ('1347', 'cat'),\n",
       "             ('2157', 'cat'),\n",
       "             ('4000', 'zebra'),\n",
       "             ('4001', 'Zebra'),\n",
       "             ('4005', 'hot dog'),\n",
       "             ('4006', 'hot dogs'),\n",
       "             ('4007', 'cow'),\n",
       "             ('3971', 'head'),\n",
       "             ('3970', 'music studio'),\n",
       "             ('3973', 'bottle'),\n",
       "             ('3972', 'chess board'),\n",
       "             ('3977', 'keyboard'),\n",
       "             ('3978', 'keyboard'),\n",
       "             ('3628', 'house'),\n",
       "             ('3621', 'refrigerator'),\n",
       "             ('3620', 'refrigerator'),\n",
       "             ('3623', 'refrigerator'),\n",
       "             ('3622', 'refrigerator'),\n",
       "             ('3627', 'house'),\n",
       "             ('3626', 'house'),\n",
       "             ('4479', 'luggage'),\n",
       "             ('4478', 'luggage'),\n",
       "             ('4470', 'pen'),\n",
       "             ('4473', 'cake'),\n",
       "             ('4475', 'Baseball'),\n",
       "             ('4474', 'plates'),\n",
       "             ('2464', 'dog'),\n",
       "             ('2460', 'bedroom'),\n",
       "             ('2461', 'dog'),\n",
       "             ('2462', 'dog'),\n",
       "             ('2463', 'dog'),\n",
       "             ('5346', 'trombone'),\n",
       "             ('5343', 'trombone'),\n",
       "             ('1644', 'bottle'),\n",
       "             ('1647', 'Motorcycles'),\n",
       "             ('1646', 'motorcycle'),\n",
       "             ('1649', 'trucks'),\n",
       "             ('573', 'trombone'),\n",
       "             ('576', 'pizza'),\n",
       "             ('574', 'play music'),\n",
       "             ('2808', 'bottle'),\n",
       "             ('2802', 'bird'),\n",
       "             ('2803', 'monitor'),\n",
       "             ('2806', 'baseball'),\n",
       "             ('2804', 'You can use a baseball'),\n",
       "             ('2805', 'Baseball'),\n",
       "             ('5433', 'bagels'),\n",
       "             ('5436', 'bagel'),\n",
       "             ('5434', 'bagel'),\n",
       "             ('5435', 'bagel'),\n",
       "             ('3365', 'handbag'),\n",
       "             ('3360', 'dogs'),\n",
       "             ('4273', 'snake'),\n",
       "             ('4271', 'motorcycle'),\n",
       "             ('4277', 'snake'),\n",
       "             ('4276', 'snake'),\n",
       "             ('4275', 'snake'),\n",
       "             ('4274', 'snake'),\n",
       "             ('4279', 'snake'),\n",
       "             ('4278', 'snake'),\n",
       "             ('1200', 'harmonica'),\n",
       "             ('2015', 'elephant'),\n",
       "             ('5557', 'fire hydrant'),\n",
       "             ('5551', 'baseball'),\n",
       "             ('5550', 'donut'),\n",
       "             ('4863', 'train'),\n",
       "             ('4861', 'laptop'),\n",
       "             ('2013', 'elephants'),\n",
       "             ('2012', 'elephant'),\n",
       "             ('5882', 'elephant'),\n",
       "             ('5881', 'dog'),\n",
       "             ('5880', 'monitor'),\n",
       "             ('5886', 'cats'),\n",
       "             ('5885', 'Boats'),\n",
       "             ('357', 'carrot is related to rabbit'),\n",
       "             ('356', 'dog'),\n",
       "             ('2190', 'motorcycle'),\n",
       "             ('3225', 'horses'),\n",
       "             ('4632', 'hot dog'),\n",
       "             ('2192', 'Pineapple'),\n",
       "             ('4630', 'hot dog'),\n",
       "             ('2258', 'baseball'),\n",
       "             ('2257', 'Baseball'),\n",
       "             ('4637', 'cheese'),\n",
       "             ('2255', 'ping-pong ball'),\n",
       "             ('2253', 'flowers'),\n",
       "             ('2252', 'vase'),\n",
       "             ('4636', 'Skateboards'),\n",
       "             ('3224', 'horse'),\n",
       "             ('4802', 'toilet'),\n",
       "             ('4804', 'sand'),\n",
       "             ('4805', 'pens'),\n",
       "             ('4806', 'pen'),\n",
       "             ('4807', 'desk'),\n",
       "             ('4808', 'less expensive than laptop'),\n",
       "             ('4809', 'laptop'),\n",
       "             ('2789', 'motorcycle'),\n",
       "             ('2788', 'motorcycle'),\n",
       "             ('2078', 'banana'),\n",
       "             ('2073', 'bananas'),\n",
       "             ('2072', 'banana'),\n",
       "             ('2071', 'banana'),\n",
       "             ('2070', 'banana'),\n",
       "             ('2077', 'banana'),\n",
       "             ('2076', 'banana'),\n",
       "             ('2075', 'banana'),\n",
       "             ('2074', 'banana'),\n",
       "             ('3221', 'trombone'),\n",
       "             ('5371', 'luggage'),\n",
       "             ('4398', 'pizza'),\n",
       "             ('4399', 'Cars'),\n",
       "             ('4396', 'pizza'),\n",
       "             ('4397', 'pizza'),\n",
       "             ('3197', 'harp'),\n",
       "             ('3196', 'harp'),\n",
       "             ('3194', 'luggage'),\n",
       "             ('3190', 'luggage'),\n",
       "             ('478', 'a toaster is'),\n",
       "             ('479', 'cat'),\n",
       "             ('3198', 'motorcycle'),\n",
       "             ('4026', 'dogs'),\n",
       "             ('4024', 'bows'),\n",
       "             ('4025', 'bow'),\n",
       "             ('4020', 'piano'),\n",
       "             ('4028', 'dogs'),\n",
       "             ('4029', 'dog'),\n",
       "             ('3999', 'zebra'),\n",
       "             ('3991', 'train'),\n",
       "             ('3990', 'seven times greater than train'),\n",
       "             ('298', 'banana'),\n",
       "             ('293', 'vase'),\n",
       "             ('290', 'A motorcycle is a two'),\n",
       "             ('291', 'pizza belongs'),\n",
       "             ('581', 'cutting'),\n",
       "             ('2446', 'bus'),\n",
       "             ('2447', 'Buses'),\n",
       "             ('2444', 'bus'),\n",
       "             ('2445', 'buses'),\n",
       "             ('2449', 'piano'),\n",
       "             ('1434', 'Broccoli'),\n",
       "             ('5482', 'Snakes'),\n",
       "             ('5404', 'orange'),\n",
       "             ('1433', 'spoon'),\n",
       "             ('1623', 'dogs'),\n",
       "             ('1622', 'motorcycle'),\n",
       "             ('1621', 'motorcycle'),\n",
       "             ('1998', 'motorcycle'),\n",
       "             ('2863', 'horse'),\n",
       "             ('1438', 'beach'),\n",
       "             ('5408', 'orange'),\n",
       "             ('1199', 'goldfish'),\n",
       "             ('3348', 'cow'),\n",
       "             ('1191', 'truck'),\n",
       "             ('1192', 'trucks'),\n",
       "             ('2406', 'bagel'),\n",
       "             ('4259', 'motorcycle'),\n",
       "             ('4258', 'sand'),\n",
       "             ('4251', 'dog'),\n",
       "             ('4250', 'pizza'),\n",
       "             ('4253', 'banana'),\n",
       "             ('4252', 'banana'),\n",
       "             ('4257', 'sand'),\n",
       "             ('4256', 'beach'),\n",
       "             ('5533', 'elephant'),\n",
       "             ('5531', 'station'),\n",
       "             ('5530', 'station'),\n",
       "             ('5537', 'camel'),\n",
       "             ('5536', 'camel'),\n",
       "             ('5535', 'camel'),\n",
       "             ('5534', 'Camel'),\n",
       "             ('5539', 'tie'),\n",
       "             ('5538', 'camel'),\n",
       "             ('1348', 'Cats'),\n",
       "             ('459', 'lemon'),\n",
       "             ('1341', 'zebra'),\n",
       "             ('3834', 'broccoli'),\n",
       "             ('3835', 'broccoli'),\n",
       "             ('3836', 'broccoli'),\n",
       "             ('3837', 'broccoli'),\n",
       "             ('3830', 'broccoli'),\n",
       "             ('3831', 'broccoli'),\n",
       "             ('3832', 'broccoli'),\n",
       "             ('3833', 'broccoli'),\n",
       "             ('3838', 'pizza'),\n",
       "             ('456', 'Apples are a fruit'),\n",
       "             ('457', 'laptop'),\n",
       "             ('379', 'microwave'),\n",
       "             ('378', 'cat'),\n",
       "             ('2275', 'computer'),\n",
       "             ('2274', 'keyboard'),\n",
       "             ('2277', 'bee'),\n",
       "             ('2276', 'computer'),\n",
       "             ('2271', 'Artichoke'),\n",
       "             ('2270', 'artichoke'),\n",
       "             ('2273', 'sound control room'),\n",
       "             ('2272', 'lemon'),\n",
       "             ('2279', 'monitor'),\n",
       "             ('2278', 'desk'),\n",
       "             ('2051', 'bakery'),\n",
       "             ('2050', 'donut'),\n",
       "             ('2059', 'goldfish'),\n",
       "             ('2058', 'goldfish'),\n",
       "             ('4824', 'horse'),\n",
       "             ('4537', 'harp'),\n",
       "             ('4531', 'boat'),\n",
       "             ('4532', 'boat'),\n",
       "             ('4821', 'laptop'),\n",
       "             ('2405', 'bagel'),\n",
       "             ('1384', 'dishes'),\n",
       "             ('1380', 'train'),\n",
       "             ('1381', 'train'),\n",
       "             ('1383', 'handbag'),\n",
       "             ('5270', 'wall'),\n",
       "             ('5272', 'luggage'),\n",
       "             ('2420', 'baseball'),\n",
       "             ('2421', 'kitchen utensil'),\n",
       "             ('2422', 'refrigerator'),\n",
       "             ('2423', 'refrigerator'),\n",
       "             ('2424', 'refrigerator'),\n",
       "             ('2425', 'microwave'),\n",
       "             ('1815', 'Horses'),\n",
       "             ('1816', 'horses'),\n",
       "             ('1817', 'horses'),\n",
       "             ('1811', 'banana'),\n",
       "             ('1818', 'Horses'),\n",
       "             ('2165', 'living room can be warmer than bedroom'),\n",
       "             ('2164', 'cow'),\n",
       "             ('2167', 'carrot'),\n",
       "             ('2166', 'room is furniture'),\n",
       "             ('3228', 'motorcycle'),\n",
       "             ('1979', 'dog'),\n",
       "             ('1978', 'dog'),\n",
       "             ('1977', 'dog'),\n",
       "             ('1600', 'lemon'),\n",
       "             ('1603', 'orange'),\n",
       "             ('1604', 'lemon'),\n",
       "             ('1606', 'baseball'),\n",
       "             ('2163', 'cat'),\n",
       "             ('2848', 'beaker'),\n",
       "             ('2846', 'cats'),\n",
       "             ('2844', 'cattle'),\n",
       "             ('2845', 'cow'),\n",
       "             ('2843', 'heifer'),\n",
       "             ('2840', 'cattle'),\n",
       "             ('2841', 'cow'),\n",
       "             ('3320', 'dogs'),\n",
       "             ('5477', 'zebra'),\n",
       "             ('3323', 'dogs'),\n",
       "             ('3324', 'pizza'),\n",
       "             ('5479', 'elephants'),\n",
       "             ('5519', 'dog'),\n",
       "             ('5518', 'dog'),\n",
       "             ('5511', 'tomato'),\n",
       "             ('5510', 'tomato'),\n",
       "             ('5515', 'dog'),\n",
       "             ('5517', 'dog'),\n",
       "             ('5516', 'dogs'),\n",
       "             ('537', 'Bananas'),\n",
       "             ('535', 'table'),\n",
       "             ('534', 'bathroom'),\n",
       "             ('533', 'A toothbrush should'),\n",
       "             ('1908', 'sand'),\n",
       "             ('1909', 'dogs'),\n",
       "             ('3294', 'dog'),\n",
       "             ('3295', 'dog'),\n",
       "             ('3296', 'dog'),\n",
       "             ('3291', 'Hot dogs'),\n",
       "             ('3292', 'Hot dogs'),\n",
       "             ('1901', 'violin'),\n",
       "             ('3810', 'vase'),\n",
       "             ('3811', 'vase'),\n",
       "             ('3819', 'camel'),\n",
       "             ('2213', 'bus'),\n",
       "             ('2212', 'bus'),\n",
       "             ('2211', 'bus'),\n",
       "             ('2217', 'laptop'),\n",
       "             ('2216', 'desk'),\n",
       "             ('2219', 'car are a lot more expensive than laptop'),\n",
       "             ('2218', 'tie'),\n",
       "             ('4518', 'jellyfish'),\n",
       "             ('4519', 'jellyfish'),\n",
       "             ('4513', 'jellyfish'),\n",
       "             ('4516', 'Jellyfish'),\n",
       "             ('4517', 'jellyfish'),\n",
       "             ('4514', 'jellyfish'),\n",
       "             ('4515', 'jellyfish'),\n",
       "             ('2989', 'violin'),\n",
       "             ('2988', 'violin'),\n",
       "             ('2987', 'violins'),\n",
       "             ('1018', 'Dogs'),\n",
       "             ('1019', 'dogs'),\n",
       "             ('1014', 'cut'),\n",
       "             ('1015', 'laptop'),\n",
       "             ('1016', 'cat'),\n",
       "             ('1017', 'dog'),\n",
       "             ('4358', 'Refrigerator'),\n",
       "             ('4359', 'Motorcycles'),\n",
       "             ('4354', 'microwave'),\n",
       "             ('4355', 'microwave'),\n",
       "             ('4356', 'microwave'),\n",
       "             ('4357', 'refrigerator'),\n",
       "             ('4350', 'toilet'),\n",
       "             ('4352', 'golfcart'),\n",
       "             ('4353', 'golfcart'),\n",
       "             ('438', 'trombone'),\n",
       "             ('434', 'airport'),\n",
       "             ('435', 'keyboard'),\n",
       "             ('431', 'pee'),\n",
       "             ('5700', 'refrigerator'),\n",
       "             ('5701', 'boats'),\n",
       "             ('5702', 'dog'),\n",
       "             ('1430', 'spoon'),\n",
       "             ('1055', 'zebra'),\n",
       "             ('1056', 'zebra'),\n",
       "             ('1057', 'Zebra'),\n",
       "             ('3796', 'keyboard'),\n",
       "             ('3790', 'banana'),\n",
       "             ('3793', 'motorcycle'),\n",
       "             ('2155', 'cats'),\n",
       "             ('2156', 'cats'),\n",
       "             ('4985', 'You are likely to find a kitchenette in a house'),\n",
       "             ('2153', 'artichoke'),\n",
       "             ('2158', 'cat'),\n",
       "             ('2159', 'cat'),\n",
       "             ('2408', 'bagels'),\n",
       "             ('2409', 'strawberries'),\n",
       "             ('1955', 'cat'),\n",
       "             ('1956', 'Cats'),\n",
       "             ('1951', 'cow'),\n",
       "             ('1950', 'chicken'),\n",
       "             ('1952', 'wall'),\n",
       "             ('1959', 'racket'),\n",
       "             ('1990', 'banana'),\n",
       "             ('5498', 'dog'),\n",
       "             ('5499', 'baseball'),\n",
       "             ('3306', 'Cats'),\n",
       "             ('3307', 'Cats'),\n",
       "             ('5496', 'Horses'),\n",
       "             ('5497', 'donut'),\n",
       "             ('3300', 'elephant'),\n",
       "             ('3301', 'tourists'),\n",
       "             ('612', '(motor'),\n",
       "             ('1274', 'cake'),\n",
       "             ('2060', 'goldfish'),\n",
       "             ('615', 'laptop'),\n",
       "             ('2063', 'bathroom'),\n",
       "             ('2064', 'Broccoli'),\n",
       "             ('2065', 'Broccoli'),\n",
       "             ('519', 'lemon'),\n",
       "             ('518', 'lemon'),\n",
       "             ('511', 'orange'),\n",
       "             ('517', 'lemon'),\n",
       "             ('1222', 'ties'),\n",
       "             ('629', 'squirrel'),\n",
       "             ('628', 'squirrel'),\n",
       "             ('3879', 'bathroom'),\n",
       "             ('3870', 'banana'),\n",
       "             ('3874', 'Bananas'),\n",
       "             ('3875', 'orange'),\n",
       "             ('2238', 'wall'),\n",
       "             ('2231', 'elephant'),\n",
       "             ('2230', 'elephant'),\n",
       "             ('2237', 'remote'),\n",
       "             ('4608', 'dog are more loyal than cat'),\n",
       "             ('3498', 'motorcycle'),\n",
       "             ('3496', 'train'),\n",
       "             ('4572', 'cake'),\n",
       "             ('4573', 'cake'),\n",
       "             ('4576', 'computer'),\n",
       "             ('4578', 'butterfly'),\n",
       "             ('4579', 'bee'),\n",
       "             ('2099', 'hot dog'),\n",
       "             ('3333', 'cat'),\n",
       "             ('3', 'butterfly'),\n",
       "             ('3331', 'dog'),\n",
       "             ('3337', 'harp'),\n",
       "             ('1655', 'cow'),\n",
       "             ('1039', 'butterfly'),\n",
       "             ('1030', 'bird'),\n",
       "             ('1031', 'pineapple'),\n",
       "             ('4378', 'motorcycle'),\n",
       "             ('4379', 'motorcycle'),\n",
       "             ('4372', 'cat'),\n",
       "             ('4373', 'cat'),\n",
       "             ('4371', 'cat'),\n",
       "             ('3335', 'harp'),\n",
       "             ('4377', 'motorcycle'),\n",
       "             ('4374', 'cat'),\n",
       "             ('4080', 'beehouse'),\n",
       "             ('4081', 'beach is romantic'),\n",
       "             ...])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ëá™ÂÆö‰πâ hit@k ËØÑ‰º∞ÂáΩÊï∞\n",
    "def hit_at_k(predictions, references, k):\n",
    "    hits = 0\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        if ref in pred[:k]:\n",
    "            hits += 1\n",
    "    return hits / len(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit@1: 43.22%\n",
      "hit@5: 66.31%\n",
      "hit@10: 66.57%\n"
     ]
    }
   ],
   "source": [
    "# ÊèêÂèñÈ¢ÑÊµãÂíåÂèÇËÄÉÁ≠îÊ°à\n",
    "predictions = []\n",
    "references = []\n",
    "for data in validation_dataset:\n",
    "    key = data['key']\n",
    "    if key in final_predictions:\n",
    "        predictions.append(final_predictions[key].split())  # Â∞ÜÈ¢ÑÊµãÁªìÊûúÂàÜÂâ≤ÊàêÂçïËØçÂàóË°®\n",
    "        references.append(data['answers']['text'][0])\n",
    "        \n",
    "# ËÆ°ÁÆó hit@5 Âíå hit@10\n",
    "hit1 = hit_at_k(predictions, references, 1)\n",
    "hit5 = hit_at_k(predictions, references, 5)\n",
    "hit10 = hit_at_k(predictions, references, 10)\n",
    "\n",
    "print(f\"hit@1: {hit1:.2%}\")\n",
    "print(f\"hit@5: {hit5:.2%}\")\n",
    "print(f\"hit@10: {hit10:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÂÖ∂‰ªñËé∑ÂèñÁ≠îÊ°àÁöÑÊñπÊ≥ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit@1: 72.68%\n",
      "hit@5: 73.49%\n",
      "hit@10: 74.71%\n"
     ]
    }
   ],
   "source": [
    "# ÊèêÂèñÈ¢ÑÊµãÂíåÂèÇËÄÉÁ≠îÊ°à\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for key, question, context, answers in zip(validation_dataset['key'], validation_dataset['question'], validation_dataset['context'], validation_dataset['answers']):\n",
    "    # È¢ÑÊµãÁ≠îÊ°à\n",
    "    inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "       outputs = trained_model(**inputs)\n",
    "       \n",
    "    answer_start_index = outputs.start_logits.argmax()\n",
    "    answer_end_index = outputs.end_logits.argmax()\n",
    "    \n",
    "    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "    predict_answer = tokenizer.decode(predict_answer_tokens, skip_special_tokens=True)\n",
    "    predictions.append(predict_answer.split())\n",
    "    references.append(answers['text'][0])\n",
    "\n",
    "# ËÆ°ÁÆó hit@5 Âíå hit@10\n",
    "hit1 = hit_at_k(predictions, references, 1)\n",
    "hit5 = hit_at_k(predictions, references, 5)\n",
    "hit10 = hit_at_k(predictions, references, 10)\n",
    "\n",
    "print(f\"hit@1: {hit1:.2%}\")\n",
    "print(f\"hit@5: {hit5:.2%}\")\n",
    "print(f\"hit@10: {hit10:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit@1: 72.68%\n",
      "hit@5: 73.49%\n",
      "hit@10: 74.71%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ëá™ÂÆö‰πâ hit@k ËØÑ‰º∞ÂáΩÊï∞\n",
    "def hit_at_k(predictions, references, k):\n",
    "    hits = 0\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        if ref in pred[:k]:\n",
    "            hits += 1\n",
    "    return hits / len(references)\n",
    "\n",
    "# ËÆ°ÁÆó hit@5 Âíå hit@10\n",
    "hit1 = hit_at_k(predictions, references, 1)\n",
    "hit5 = hit_at_k(predictions, references, 5)\n",
    "hit10 = hit_at_k(predictions, references, 10)\n",
    "\n",
    "print(f\"hit@1: {hit1:.2%}\")\n",
    "print(f\"hit@5: {hit5:.2%}\")\n",
    "print(f\"hit@10: {hit10:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Openclip Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, _, clip_preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k', device='cuda')\n",
    "# vit_tokenizer = open_clip.get_tokenizer('ViT-B-32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key': '270',\n",
       " 'question': 'Which object can be found in a jazz club',\n",
       " 'context': 'You are likely to find a trumpet in a jazz club',\n",
       " 'answers': {'answer_start': [25], 'text': ['trumpet']},\n",
       " 'image_name': '/root/autodl-tmp/vqa/VQA-with-XProNet/data/KG_VQA/fvqa/exp_data/images/images/ILSVRC2012_test_00050748.JPEG'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 512]), torch.Size([1, 512]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open(train_dataset[0]['image_name']).convert(\"RGB\")\n",
    "image_input = clip_preprocess(image).unsqueeze(0).to(device)  # Unsqueeze Ê∑ªÂä†‰∏Ä‰∏™ÊâπÊ¨°Áª¥Â∫¶\n",
    "text_tokens = clip_tokenizer.tokenize(train_dataset[0]['context']).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = clip_model.encode_image(image_input).float()\n",
    "    text_features = clip_model.encode_text(text_tokens).float()\n",
    "    \n",
    "image_features.shape, text_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ëé∑Âèñ bert ÂµåÂÖ•Â±Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "qa_bert = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output_hidden_states (bool, optional) ‚Äî Whether or not to return the hidden states of all layers. See hidden_states under returned tensors for more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÂáÜÂ§áËæìÂÖ•Êï∞ÊçÆ\n",
    "input_ids = tokenizer.encode(train_dataset['question'][0],  train_dataset['context'][0], add_special_tokens=False, return_tensors=\"pt\")\n",
    "# Ëé∑ÂèñÂµåÂÖ•Â±ÇËæìÂá∫\n",
    "with torch.no_grad():\n",
    "    # Ë∞ÉÁî®Ê®°ÂûãÔºåÁ°Æ‰øùËØ∑Ê±ÇÈöêËóèÁä∂ÊÄÅ\n",
    "    outputs = qa_bert(input_ids, output_hidden_states=True)\n",
    "    # Ëé∑Âèñ last_hidden_state\n",
    "    last_hidden_state = outputs.hidden_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25,\n",
       " tensor([[2029, 4874, 2064, 2022, 2179, 1999, 1037, 4166, 2252, 2017, 2024, 3497,\n",
       "          2000, 2424, 1037, 9368, 1999, 1037, 4166, 2252]]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs.hidden_states), input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 20, 1024]),\n",
       " tensor([[[ 0.2163, -0.2789, -1.2007,  ...,  0.0475,  0.8559, -0.8639],\n",
       "          [-0.7926, -0.3257, -0.0169,  ...,  0.9857,  1.0101, -1.0077],\n",
       "          [-1.0194,  0.0947,  0.0371,  ...,  1.0660,  0.6618, -0.1563],\n",
       "          ...,\n",
       "          [-1.4010, -0.9902, -0.1381,  ...,  0.6080,  0.8213, -0.9767],\n",
       "          [-1.3012, -0.9542, -0.0553,  ...,  0.3923,  0.6864, -1.2293],\n",
       "          [-1.3286, -0.7271, -0.7457,  ...,  0.6030,  1.1488, -0.9676]]]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state.shape, last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÊãºÊé•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 2048])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Êâ©Â±ï image_features Âíå text_features Âà∞ [1, 20, 512]\n",
    "image_features_expanded = image_features.repeat(1, 20, 1).to(device)\n",
    "text_features_expanded = text_features.repeat(1, 20, 1).to(device)\n",
    "\n",
    "# ÊãºÊé•Ëøô‰∫õÁâπÂæÅ\n",
    "combined_features = torch.cat([last_hidden_state.to(device), image_features_expanded, text_features_expanded], dim=2)  # ÊãºÊé•Âú®ÁâπÂæÅÁª¥Â∫¶\n",
    "combined_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ËæìÂÖ•ÂµåÂÖ•ÂêëÈáèÂà∞ qabert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs_embeds (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size), optional) ‚Äî Optionally, instead of passing input_ids you can choose to directly pass an embedded representation. This is useful if you want more control over how to convert input_ids indices into associated vectors than the model‚Äôs internal embedding lookup matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-2.7434, -1.8820, -3.2943, -3.1643, -3.0274, -3.2030, -3.0887, -3.0251,\n",
       "         -1.3093, -3.1568, -3.4538, -3.1751, -3.3870, -3.2753, -3.1460, -2.0125,\n",
       "         -2.8449, -2.3112, -2.6830, -2.0979]], device='cuda:0',\n",
       "       grad_fn=<CloneBackward0>), end_logits=tensor([[-2.1292, -1.1948, -2.5703, -2.5366, -2.4104, -2.6203, -2.4670, -2.2586,\n",
       "         -0.5762, -2.2948, -2.5812, -2.4727, -2.5212, -2.4964, -2.3861, -1.4723,\n",
       "         -2.2931, -1.6980, -1.9327, -1.5011]], device='cuda:0',\n",
       "       grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ÂàõÂª∫‰∏Ä‰∏™Á∫øÊÄßÂ±ÇÔºåÂ∞ÜÁâπÂæÅ‰ªé 2048 ÈôçÁª¥Âà∞ 1024\n",
    "fc = nn.Linear(combined_features.shape[-1], 1024).to(device)\n",
    "# Â∫îÁî®Á∫øÊÄßÂ±Ç\n",
    "final_features = fc(combined_features).to(device)\n",
    "qa_bert.to(device)\n",
    "qa_bert(inputs_embeds=final_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQA train Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "qa_bert = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "qa_bert = qa_bert.to(device)\n",
    "clip_model, _, clip_preprocess = open_clip.create_model_and_transforms('ViT-B-16', pretrained='openai', device=device)\n",
    "# clip_model, _, clip_preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k', device=device)\n",
    "# clip_model, _, clip_preprocess = open_clip.create_model_and_transforms('convnext_base_w', pretrained='laion2b_s13b_b82k_augreg', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = Image.open(train_dataset[0]['image_name']).convert(\"RGB\")\n",
    "# image_input = clip_preprocess(image).unsqueeze(0).to(device)  # Unsqueeze Ê∑ªÂä†‰∏Ä‰∏™ÊâπÊ¨°Áª¥Â∫¶\n",
    "# text_tokens = clip_tokenizer.tokenize(train_dataset[0]['context']).to(device)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     image_features = clip_model.encode_image(image_input).float()\n",
    "#     text_features = clip_model.encode_text(text_tokens).float()\n",
    "    \n",
    "# image_features.shape, text_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_combine_data(batch):\n",
    "    # Â§ÑÁêÜ‰∏ÄÊâπÂõæÂÉèÊï∞ÊçÆ\n",
    "    images = [Image.open(path).convert(\"RGB\") for path in batch['image_name']]\n",
    "    image_inputs = torch.stack([clip_preprocess(image) for image in images]).to(device)\n",
    "\n",
    "    # Â§ÑÁêÜ‰∏ÄÊâπÊñáÊú¨Êï∞ÊçÆ\n",
    "    text_tokens = clip_tokenizer.tokenize([context for context in batch['context']]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(image_inputs).float()\n",
    "        # print(image_features.shape)\n",
    "        text_features = clip_model.encode_text(text_tokens).float()\n",
    "        # print(text_features.shape)\n",
    "\n",
    "    # ÂáÜÂ§áËæìÂÖ•Êï∞ÊçÆ\n",
    "    # input_ids = tokenizer.encode(batch['question'], batch['context'], add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "    # Â§ÑÁêÜ‰∏ÄÊâπÈóÆÈ¢òÂíå‰∏ä‰∏ãÊñáÊï∞ÊçÆ\n",
    "    # input_ids = [tokenizer.encode(q, c, add_special_tokens=True, return_tensors=\"pt\") for q, c in zip(batch['question'], batch['context'])]\n",
    "    input_ids = [tokenizer.encode(q, c, add_special_tokens=True, return_tensors=\"pt\", max_length=max_length, padding='max_length', truncation=True) for q, c in zip(batch['question'], batch['context'])]\n",
    "    input_ids = torch.cat(input_ids, dim=0).to(device)  # ÂêàÂπ∂ÊâπÊ¨°Êï∞ÊçÆ\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = qa_bert(input_ids, output_hidden_states=True)\n",
    "        last_hidden_states = outputs.hidden_states[-1]\n",
    "        # print(last_hidden_states.shape)\n",
    "\n",
    "    # Êâ©Â±ïÂõæÂÉèÁâπÂæÅÂíåÊñáÊú¨ÁâπÂæÅ\n",
    "    image_features_expanded = image_features.unsqueeze(1).repeat(1, last_hidden_states.shape[1], 1).to(device)\n",
    "    # print(image_features_expanded.shape)\n",
    "    text_features_expanded = text_features.unsqueeze(1).repeat(1, last_hidden_states.shape[1], 1).to(device)\n",
    "    # print(text_features_expanded.shape)\n",
    "\n",
    "    # ÊãºÊé•ÁâπÂæÅ\n",
    "    combined_features = torch.cat([last_hidden_states.to(device), image_features_expanded, text_features_expanded], dim=2)\n",
    "\n",
    "    return {\"combined_features\": combined_features}\n",
    "    # return {\"combined_features\": combined_features.cpu().numpy()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'combined_features': tensor([[[ 1.4703, -1.1017,  0.0225,  ..., -0.3972, -0.2359,  0.1485],\n",
       "          [-0.1402, -1.5653,  0.3490,  ..., -0.3972, -0.2359,  0.1485],\n",
       "          [-0.3292, -1.4199,  0.1754,  ..., -0.3972, -0.2359,  0.1485],\n",
       "          ...,\n",
       "          [ 1.2099, -0.9790,  0.1319,  ..., -0.3972, -0.2359,  0.1485],\n",
       "          [ 1.1496, -1.0239,  0.1406,  ..., -0.3972, -0.2359,  0.1485],\n",
       "          [ 1.1948, -0.9850,  0.0982,  ..., -0.3972, -0.2359,  0.1485]],\n",
       " \n",
       "         [[ 1.3351, -1.0875, -0.0421,  ..., -0.2256,  0.3205, -0.0110],\n",
       "          [ 0.0961, -1.3786,  0.3776,  ..., -0.2256,  0.3205, -0.0110],\n",
       "          [ 0.2673, -0.9926, -0.0607,  ..., -0.2256,  0.3205, -0.0110],\n",
       "          ...,\n",
       "          [ 1.2144, -0.9850,  0.0387,  ..., -0.2256,  0.3205, -0.0110],\n",
       "          [ 1.1847, -1.0080,  0.0383,  ..., -0.2256,  0.3205, -0.0110],\n",
       "          [ 1.1952, -0.9801,  0.0129,  ..., -0.2256,  0.3205, -0.0110]]],\n",
       "        device='cuda:0')}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = process_combine_data(train_dataset[:2])\n",
    "features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÈöèÊú∫ÁîüÊàê100‰∏™ÂîØ‰∏ÄÁöÑÁ¥¢Âºï\n",
    "indices = np.random.permutation(len(train_dataset))[:100]\n",
    "# ‰ΩøÁî®ÈÄâÂÆöÁöÑÁ¥¢ÂºïÂàáÂâ≤Êï∞ÊçÆÈõÜ\n",
    "small_train_dataset = train_dataset.select(indices)\n",
    "\n",
    "indices = np.random.permutation(len(validation_dataset))[:20]\n",
    "small_val_dataset = validation_dataset.select(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/100 [00:00<?, ? examples/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 20.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Â∫îÁî®ÂáΩÊï∞Âà∞Êï∞ÊçÆÈõÜÁöÑÊØè‰∏ÄÊâπÊï∞ÊçÆ\n",
    "# NOTEÔºöÂÖà‰ΩøÁî®Â∞ëÈáèÊï∞ÊçÆËøõË°åÊµãËØï\n",
    "processed_train_dataset = small_train_dataset.map(process_combine_data, batched=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 22.44 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['key', 'question', 'context', 'answers', 'image_name', 'combined_features'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_val_dataset = small_val_dataset.map(process_combine_data, batched=True, batch_size=32)\n",
    "processed_val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 220.41 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 392.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Â§ÑÁêÜÊï∞ÊçÆÈõÜ\n",
    "tokenized_combine_train_dataset = processed_train_dataset.map(prepare_train_features, batched=True, remove_columns=['key', 'question', 'context', 'answers', 'image_name'])\n",
    "tokenized_combine_val_dataset = processed_val_dataset.map(prepare_train_features, batched=True, remove_columns=['key', 'question', 'context', 'answers', 'image_name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['combined_features', 'input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_combine_val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.fc = nn.Linear(2048, 1024)  # ÊãºÊé•ÂêéÁöÑÁâπÂæÅÁª¥Â∫¶‰∏∫ 2048,Â∞ÜÂÖ∂ÈôçÁª¥Âà∞ 1024\n",
    "\n",
    "    def forward(self, combined_features, start_positions, end_positions):\n",
    "        reduced_features = self.fc(combined_features)\n",
    "        outputs = self.bert(inputs_embeds=reduced_features, start_positions=start_positions, end_positions=end_positions)\n",
    "        return outputs.loss, outputs.start_logits, outputs.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/vqa/VQA-with-XProNet/myvqa/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "WARNING:accelerate.utils.other:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:41, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.374900</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7, training_loss=6.259158406938825, metrics={'train_runtime': 56.5143, 'train_samples_per_second': 1.769, 'train_steps_per_second': 0.124, 'total_flos': 0.0, 'train_loss': 6.259158406938825, 'epoch': 1.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqa_model = CustomModel(qa_bert).to(device)  # Á°Æ‰øùÊ®°Âûã‰πüÂú®Ê≠£Á°ÆÁöÑËÆæÂ§á‰∏ä\n",
    "# Ëá™ÂÆö‰πâÊï∞ÊçÆÊî∂ÈõÜÂô®\n",
    "# data_collator = MyDataCollator()\n",
    "\n",
    "# ËÆæÁΩÆËÆ≠ÁªÉÂèÇÊï∞\n",
    "model_name = \"vqa-bert\"\n",
    "batch_size = 16\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-squad\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=0.001,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=5\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=vqa_model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_combine_train_dataset,\n",
    "    eval_dataset=tokenized_combine_val_dataset,\n",
    "    data_collator=default_data_collator,  \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQA + Prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototype Matrix Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_matrix shape: (100, 384, 2048)\n",
      "features_matrix shape: (100, 2048)\n",
      "(10, 2048)\n",
      "Prototype vectors:\n",
      " [[ 1.19071023e+00 -9.64432258e-01  1.30624041e-04 ...  4.99812750e-02\n",
      "  -8.89559196e-02  7.53895827e-02]\n",
      " [-6.18201035e-01 -8.81037543e-01 -2.01249824e-01 ... -2.08062132e-02\n",
      "  -1.51775781e-01  7.27241635e-02]\n",
      " [ 1.18580058e+00 -9.63028422e-01 -1.37879116e-03 ...  4.00370925e-02\n",
      "   2.01106527e-02  2.19146271e-01]\n",
      " ...\n",
      " [ 1.14892395e+00 -1.00487485e+00  4.84245156e-02 ... -4.68075089e-03\n",
      "  -4.69022592e-02 -1.44994088e-02]\n",
      " [ 1.18890511e+00 -9.72748384e-01 -1.28681172e-02 ...  2.22443674e-01\n",
      "  -3.89508198e-03  2.30963608e-02]\n",
      " [ 1.17891174e+00 -9.66156564e-01 -6.33837884e-03 ...  1.41137085e-01\n",
      "  -7.55139544e-02  1.41927724e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# ÂÅáËÆæ processed_train_dataset ÊòØÂ∑≤ÁªèÂä†ËΩΩÁöÑÊï∞ÊçÆÈõÜ\n",
    "features = processed_train_dataset['combined_features']\n",
    "\n",
    "# Â∞ÜÁâπÂæÅÂàóË°®ËΩ¨Êç¢‰∏∫ NumPy Êï∞ÁªÑÔºå‰ª•‰æøÁî®‰∫é K-Means ÁÆóÊ≥ï\n",
    "features_matrix = np.array(features)\n",
    "print(f\"features_matrix shape: {features_matrix.shape}\")\n",
    "\n",
    "# features_matrix = features_matrix.reshape(features_matrix.shape[1], -1)\n",
    "features_matrix = features_matrix.mean(axis=1)\n",
    "print(f\"features_matrix shape: {features_matrix.shape}\")\n",
    "\n",
    "# ËÆæÁΩÆ K-Means ÁÆóÊ≥ïÔºåËÅöÁ±ªÊï∞‰∏∫ 10\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(features_matrix)\n",
    "\n",
    "# Ëé∑ÂèñÊØè‰∏™Á∞áÁöÑ‰∏≠ÂøÉÁÇπÔºåËøô‰∫õ‰∏≠ÂøÉÁÇπÂç≥‰∏∫ÂéüÂûãÂêëÈáè\n",
    "prototype_vectors = kmeans.cluster_centers_\n",
    "print(prototype_vectors.shape)\n",
    "# ÊâìÂç∞ÂéüÂûãÂêëÈáèÔºåÊü•ÁúãÁªìÊûú\n",
    "print(\"Prototype vectors:\\n\", prototype_vectors)\n",
    "prototype_vectors = torch.tensor(prototype_vectors, dtype=torch.float32)\n",
    "prototype_vectors = prototype_vectors.unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototype Querying And Responsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Prototype(nn.Module):\n",
    "#     def __init__(self, prototype_vectors, feature_dim=2048, num_prototypes=3):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             prototype_vectors (np.ndarray): ÂéüÂûãÂêëÈáèÔºàn_clustersÔºå 2048Ôºâ\n",
    "#             feature_dim (int): ËΩ¨Êç¢ÁõÆÊ†áÁâπÂæÅÁª¥Â∫¶\n",
    "#             num_prototypes (int): ÊØè‰∏™ÁâπÂæÅÈÄâÊã©ÁöÑ topk Áõ∏‰ººÂéüÂûãÊï∞Èáè\n",
    "#         \"\"\"\n",
    "#         super(Prototype, self).__init__()\n",
    "#         self.prototypes = nn.Parameter(torch.tensor(prototype_vectors, dtype=torch.float32), requires_grad=False)\n",
    "#         self.dim_reduction = nn.Linear(prototype_vectors.shape[1], feature_dim) # Áî®‰∫éÈôçÁª¥ÂéüÂûãÂêëÈáèÂà∞‰∏écombined_featuresÁõ∏ÂêåÁöÑÁª¥Â∫¶\n",
    "#         self.num_prototypes = num_prototypes\n",
    "\n",
    "#     def forward(self, combined_features):\n",
    "#         # print(f\"combined_features: {combined_features.shape}\")\n",
    "#         # Á∫øÊÄßÊäïÂΩ±\n",
    "#         projected_features = self.dim_reduction(self.prototypes)\n",
    "#         # print(f\"projected_features: {projected_features.shape}\")\n",
    "        \n",
    "#         projected_features = projected_features.unsqueeze(1)\n",
    "#         print(f\"projected_features: {projected_features.shape}\")\n",
    "        \n",
    "#         # combined_features = combined_features.unsqueeze(0)\n",
    "#         # print(f\"combined_features: {combined_features.shape}\")\n",
    "        \n",
    "#         # Áõ∏‰ººÂ∫¶ËÆ°ÁÆóÔºà‰ΩôÂº¶Áõ∏‰ººÂ∫¶Ôºâ\n",
    "#         similarity = F.cosine_similarity(projected_features, combined_features, dim=2)\n",
    "#         similarity = similarity.unsqueeze(0)\n",
    "#         print(f\"similarity: {similarity.shape}\")\n",
    "#         # ÂéüÂûãÈÄâÊã©ÔºöÈÄâÊã©ÊØè‰∏™ÁâπÂæÅÊúÄÁõ∏ÂÖ≥ÁöÑÂâç num_prototypes ‰∏™ÂéüÂûã\n",
    "#         topk_vals, topk_indices = torch.topk(similarity, self.num_prototypes, dim=1)\n",
    "#         # ÊùÉÈáçËÆ°ÁÆóÔºöËøôÈáå‰ΩøÁî® softmax Êù•ÂΩí‰∏ÄÂåñÊùÉÈáç\n",
    "#         weights = F.softmax(topk_vals, dim=2)\n",
    "#         # print(f\"weights: {weights.shape}\")\n",
    "#         # ÂìçÂ∫îÁîüÊàêÔºöÂä†ÊùÉÊ±ÇÂíåÂéüÂûãÂêëÈáè\n",
    "#         selected_prototypes = self.prototypes[topk_indices.squeeze(0)] \n",
    "#         # print(f\"selected_prototypes: {selected_prototypes.shape}\")\n",
    "#         response = torch.sum(weights.unsqueeze(-1) * selected_prototypes, dim=1)\n",
    "#         return response\n",
    "\n",
    "# class Prototype(nn.Module):\n",
    "#     def __init__(self, prototype_vectors, feature_dim=2048, num_prototypes=3):\n",
    "#         super(Prototype, self).__init__()\n",
    "#         self.prototypes = nn.Parameter(torch.tensor(prototype_vectors, dtype=torch.float32), requires_grad=False)\n",
    "#         self.dim_reduction = nn.Linear(prototype_vectors.shape[1], feature_dim)\n",
    "#         self.num_prototypes = num_prototypes\n",
    "\n",
    "#     def forward(self, combined_features):\n",
    "#         # Á∫øÊÄßÊäïÂΩ±\n",
    "#         projected_features = self.dim_reduction(self.prototypes)  # [num_prototypes, feature_dim]\n",
    "#         projected_features = projected_features.unsqueeze(0)  # [1, num_prototypes, feature_dim]\n",
    "        \n",
    "#         # Êâ©Â±ïÊâπÂ§ÑÁêÜÂ∞∫ÂØ∏\n",
    "#         projected_features = projected_features.expand(combined_features.size(0), -1, -1)  # [batch_size, num_prototypes, feature_dim]\n",
    "        \n",
    "#         # Áõ∏‰ººÂ∫¶ËÆ°ÁÆóÔºà‰ΩôÂº¶Áõ∏‰ººÂ∫¶Ôºâ\n",
    "#         similarity = F.cosine_similarity(projected_features, combined_features.unsqueeze(1), dim=2)  # [batch_size, num_prototypes, num_features]\n",
    "        \n",
    "#         # ÂéüÂûãÈÄâÊã©ÔºöÈÄâÊã©ÊØè‰∏™ÁâπÂæÅÊúÄÁõ∏ÂÖ≥ÁöÑÂâç num_prototypes ‰∏™ÂéüÂûã\n",
    "#         topk_vals, topk_indices = torch.topk(similarity, self.num_prototypes, dim=1)\n",
    "        \n",
    "#         # ÊùÉÈáçËÆ°ÁÆóÔºöËøôÈáå‰ΩøÁî® softmax Êù•ÂΩí‰∏ÄÂåñÊùÉÈáç\n",
    "#         weights = F.softmax(topk_vals, dim=1)  # [batch_size, num_prototypes, num_features]\n",
    "\n",
    "#         # ÂìçÂ∫îÁîüÊàêÔºöÂä†ÊùÉÊ±ÇÂíåÂéüÂûãÂêëÈáè\n",
    "#         selected_prototypes = self.prototypes[topk_indices]  # [batch_size, num_features, num_prototypes, feature_dim]\n",
    "#         response = torch.sum(weights.unsqueeze(-1) * selected_prototypes, dim=2)  # [batch_size, num_features, feature_dim]\n",
    "#         return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÂÅáËÆæÊúâ‰∏Ä‰∏™ËæìÂÖ•ÁâπÂæÅ\n",
    "combined_features = torch.tensor(processed_train_dataset[0]['combined_features'])\n",
    "prototype_vectors = kmeans.cluster_centers_\n",
    "prototype_vectors = torch.tensor(prototype_vectors, dtype=torch.float32)\n",
    "\n",
    "combined_features = combined_features.unsqueeze(0).to(device).float() \n",
    "prototype_vectors = prototype_vectors.unsqueeze(0).to(device).float()  \n",
    "print(combined_features.shape, prototype_vectors.shape)\n",
    "mtm = MultiThreadMemory(h=16, d_model=2048, topk=5, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = mtm(combined_features, prototype_vectors, prototype_vectors)\n",
    "output.shape, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "# qa_bert = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "# qa_bert = qa_bert.to(device)\n",
    "# clip_model, _, clip_preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k', device=device)\n",
    "\n",
    "prototype_vectors = kmeans.cluster_centers_\n",
    "prototype_vectors = torch.tensor(prototype_vectors, dtype=torch.float32)\n",
    "prototype_vectors = prototype_vectors.unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VqaPrototypeModel(nn.Module):\n",
    "    def __init__(self, prototype_vectors, device, batch_size=16, seq_length=38):\n",
    "        super(VqaPrototypeModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.bert = qa_bert.to(self.device)\n",
    "        self.prototype = MultiThreadMemory(h=16, d_model=2048, topk=3, dropout=0.1, device=self.device).to(self.device)\n",
    "        self.prototype_vectors = prototype_vectors.repeat(batch_size, seq_length, 1).to(self.device)\n",
    "        # print(f\"prototype_vectors shape: {prototype_vectors.shape}\")\n",
    "        self.fc = nn.Linear(4096, 1024).to(self.device)  # Ê≥®ÊÑèÔºåÂ¶ÇÊûúÂéüÂßãÁâπÂæÅÂíåÂìçÂ∫îË¢´ÊãºÊé•ÔºåËøôÈáåÁöÑËæìÂÖ•Áª¥Â∫¶Â∫î‰∏∫ 2048 + feature_dim\n",
    "\n",
    "    def forward(self, combined_features, start_positions, end_positions):\n",
    "        combined_features = torch.tensor(combined_features).to(self.device)\n",
    "        # print(f\"combined_features shape: {combined_features.shape}\")\n",
    "\n",
    "        response = self.prototype(combined_features, self.prototype_vectors, self.prototype_vectors, device=self.device)\n",
    "        # print(\"Shape of Response:\", response.shape)\n",
    "        \n",
    "        # ÊãºÊé•ÂéüÂûãÂìçÂ∫îÂíå BERT ËæìÂá∫\n",
    "        final_combined_features = torch.cat([combined_features, response], dim=2)\n",
    "        # print(\"Shape of final_combined_features:\", final_combined_features.shape)\n",
    "        \n",
    "        reduced_features = self.fc(final_combined_features)\n",
    "        # print(\"Shape of reduced_features:\", reduced_features.shape)\n",
    "        \n",
    "        outputs = self.bert(inputs_embeds=reduced_features, start_positions=start_positions, end_positions=end_positions)\n",
    "        \n",
    "        return outputs.loss, outputs.start_logits, outputs.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vqa_model = VqaPrototypeModel(prototype_vectors).to(device) \n",
    "# test_input = torch.tensor(tokenized_combine_train_dataset[0]['combined_features'], dtype=torch.float32)\n",
    "# test_input = test_input.unsqueeze(0).to(device)\n",
    "# print(test_input.shape)\n",
    "\n",
    "# output = vqa_model(test_input, torch.tensor([0]).to(device), torch.tensor([0]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "vqa_model = VqaPrototypeModel(prototype_vectors, device=device).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/vqa/VQA-with-XProNet/myvqa/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "WARNING:accelerate.utils.other:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2594/1055261747.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  combined_features = torch.tensor(combined_features).to(self.device)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 26\u001b[0m\n\u001b[1;32m      5\u001b[0m args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      6\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-finetuned-squad\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     gradient_accumulation_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     18\u001b[0m     model\u001b[38;5;241m=\u001b[39mvqa_model,\n\u001b[1;32m     19\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer\n\u001b[1;32m     24\u001b[0m )\n\u001b[0;32m---> 26\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/autodl-tmp/vqa/VQA-with-XProNet/myvqa/lib/python3.10/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/autodl-tmp/vqa/VQA-with-XProNet/myvqa/lib/python3.10/site-packages/transformers/trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/autodl-tmp/vqa/VQA-with-XProNet/myvqa/lib/python3.10/site-packages/transformers/trainer.py:3238\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3238\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3241\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[0;32m~/autodl-tmp/vqa/VQA-with-XProNet/myvqa/lib/python3.10/site-packages/transformers/trainer.py:3264\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3263\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3264\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3265\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3266\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/autodl-tmp/vqa/VQA-with-XProNet/myvqa/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/autodl-tmp/vqa/VQA-with-XProNet/myvqa/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 25\u001b[0m, in \u001b[0;36mVqaPrototypeModel.forward\u001b[0;34m(self, combined_features, start_positions, end_positions)\u001b[0m\n\u001b[1;32m     22\u001b[0m reduced_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(final_combined_features)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# print(\"Shape of reduced_features:\", reduced_features.shape)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduced_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_positions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_positions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_positions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_positions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mloss, outputs\u001b[38;5;241m.\u001b[39mstart_logits, outputs\u001b[38;5;241m.\u001b[39mend_logits\n",
      "File \u001b[0;32m~/autodl-tmp/vqa/VQA-with-XProNet/myvqa/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/autodl-tmp/vqa/VQA-with-XProNet/myvqa/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/autodl-tmp/vqa/VQA-with-XProNet/myvqa/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1973\u001b[0m, in \u001b[0;36mBertForQuestionAnswering.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1961\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1962\u001b[0m \u001b[38;5;124;03mstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;124;03m    Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1969\u001b[0m \u001b[38;5;124;03m    are not taken into account for computing the loss.\u001b[39;00m\n\u001b[1;32m   1970\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1971\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1973\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1974\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1979\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1980\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1981\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1983\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1985\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1987\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqa_outputs(sequence_output)\n",
      "File \u001b[0;32m~/autodl-tmp/vqa/VQA-with-XProNet/myvqa/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/autodl-tmp/vqa/VQA-with-XProNet/myvqa/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/autodl-tmp/vqa/VQA-with-XProNet/myvqa/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1137\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1137\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1149\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1150\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/autodl-tmp/vqa/VQA-with-XProNet/myvqa/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/autodl-tmp/vqa/VQA-with-XProNet/myvqa/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/autodl-tmp/vqa/VQA-with-XProNet/myvqa/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:690\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    679\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    680\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    681\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    687\u001b[0m         output_attentions,\n\u001b[1;32m    688\u001b[0m     )\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 690\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/autodl-tmp/vqa/VQA-with-XProNet/myvqa/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/autodl-tmp/vqa/VQA-with-XProNet/myvqa/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/autodl-tmp/vqa/VQA-with-XProNet/myvqa/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:622\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    619\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    620\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 622\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    625\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    627\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/autodl-tmp/vqa/VQA-with-XProNet/myvqa/lib/python3.10/site-packages/transformers/pytorch_utils.py:238\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/autodl-tmp/vqa/VQA-with-XProNet/myvqa/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:634\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 634\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/autodl-tmp/vqa/VQA-with-XProNet/myvqa/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/autodl-tmp/vqa/VQA-with-XProNet/myvqa/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/autodl-tmp/vqa/VQA-with-XProNet/myvqa/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:535\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    534\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m--> 535\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_act_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/autodl-tmp/vqa/VQA-with-XProNet/myvqa/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/autodl-tmp/vqa/VQA-with-XProNet/myvqa/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/autodl-tmp/vqa/VQA-with-XProNet/myvqa/lib/python3.10/site-packages/transformers/activations.py:78\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "# ËÆæÁΩÆËÆ≠ÁªÉÂèÇÊï∞\n",
    "model_name = \"vqa-bert\"\n",
    "batch_size = 16\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-squad\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=5,\n",
    "    gradient_accumulation_steps=4,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=vqa_model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_combine_train_dataset,\n",
    "    eval_dataset=tokenized_combine_val_dataset,\n",
    "    data_collator=default_data_collator,  \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
